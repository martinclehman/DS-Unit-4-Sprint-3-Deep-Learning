{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Architecture (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "### Extremely Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,)) # do flat representation of image\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim,activation='sigmoid')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784,activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img,decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "\n",
    "# create the decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4621 - val_loss: 0.3297\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3048 - val_loss: 0.2898\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2836 - val_loss: 0.2788\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2763 - val_loss: 0.2739\n",
      "Epoch 5/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2727 - val_loss: 0.2712\n",
      "Epoch 6/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2705 - val_loss: 0.2695\n",
      "Epoch 7/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2691 - val_loss: 0.2683\n",
      "Epoch 8/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2681 - val_loss: 0.2674\n",
      "Epoch 9/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2674 - val_loss: 0.2668\n",
      "Epoch 10/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2668 - val_loss: 0.2663\n",
      "Epoch 11/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2664 - val_loss: 0.2659\n",
      "Epoch 12/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2660 - val_loss: 0.2656\n",
      "Epoch 13/1000\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2657 - val_loss: 0.2653\n",
      "Epoch 14/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2654 - val_loss: 0.2651\n",
      "Epoch 15/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2652 - val_loss: 0.2649\n",
      "Epoch 16/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2651 - val_loss: 0.2647\n",
      "Epoch 17/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2649 - val_loss: 0.2645\n",
      "Epoch 18/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2648 - val_loss: 0.2644\n",
      "Epoch 19/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2646 - val_loss: 0.2643\n",
      "Epoch 20/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2645 - val_loss: 0.2642\n",
      "Epoch 21/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2644 - val_loss: 0.2641\n",
      "Epoch 22/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2643 - val_loss: 0.2640\n",
      "Epoch 23/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2643 - val_loss: 0.2639\n",
      "Epoch 24/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2642 - val_loss: 0.2639\n",
      "Epoch 25/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2641 - val_loss: 0.2638\n",
      "Epoch 26/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2641 - val_loss: 0.2637\n",
      "Epoch 27/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2640 - val_loss: 0.2637\n",
      "Epoch 28/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2640 - val_loss: 0.2637\n",
      "Epoch 29/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2639 - val_loss: 0.2636\n",
      "Epoch 30/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2639 - val_loss: 0.2636\n",
      "Epoch 31/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2638 - val_loss: 0.2635\n",
      "Epoch 32/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2638 - val_loss: 0.2635\n",
      "Epoch 33/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2638 - val_loss: 0.2635\n",
      "Epoch 34/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2637 - val_loss: 0.2634\n",
      "Epoch 35/1000\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.2637 - val_loss: 0.2634\n",
      "Epoch 36/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2637 - val_loss: 0.2634\n",
      "Epoch 37/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2636 - val_loss: 0.2633\n",
      "Epoch 38/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2636 - val_loss: 0.2633\n",
      "Epoch 39/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2636 - val_loss: 0.2633\n",
      "Epoch 40/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2636 - val_loss: 0.2633\n",
      "Epoch 41/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2635 - val_loss: 0.2632\n",
      "Epoch 42/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2635 - val_loss: 0.2632\n",
      "Epoch 43/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2635 - val_loss: 0.2632\n",
      "Epoch 44/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2635 - val_loss: 0.2632\n",
      "Epoch 45/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2634 - val_loss: 0.2631\n",
      "Epoch 46/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2634 - val_loss: 0.2631\n",
      "Epoch 47/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2634 - val_loss: 0.2631\n",
      "Epoch 48/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2634 - val_loss: 0.2631\n",
      "Epoch 49/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2634 - val_loss: 0.2631\n",
      "Epoch 50/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2633 - val_loss: 0.2630\n",
      "Epoch 51/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2633 - val_loss: 0.2630\n",
      "Epoch 52/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2633 - val_loss: 0.2629\n",
      "Epoch 53/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2632 - val_loss: 0.2628\n",
      "Epoch 54/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2630 - val_loss: 0.2626\n",
      "Epoch 55/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2626 - val_loss: 0.2622\n",
      "Epoch 56/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2622 - val_loss: 0.2617\n",
      "Epoch 57/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2618 - val_loss: 0.2613\n",
      "Epoch 58/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2614 - val_loss: 0.2610\n",
      "Epoch 59/1000\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2610 - val_loss: 0.2606\n",
      "Epoch 60/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2607 - val_loss: 0.2602\n",
      "Epoch 61/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2603 - val_loss: 0.2598\n",
      "Epoch 62/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2599 - val_loss: 0.2595\n",
      "Epoch 63/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2596 - val_loss: 0.2591\n",
      "Epoch 64/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2592 - val_loss: 0.2587\n",
      "Epoch 65/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2588 - val_loss: 0.2584\n",
      "Epoch 66/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2585 - val_loss: 0.2580\n",
      "Epoch 67/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2581 - val_loss: 0.2576\n",
      "Epoch 68/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2578 - val_loss: 0.2573\n",
      "Epoch 69/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2574 - val_loss: 0.2569\n",
      "Epoch 70/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2571 - val_loss: 0.2566\n",
      "Epoch 71/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2567 - val_loss: 0.2562\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2563 - val_loss: 0.2558\n",
      "Epoch 73/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2559 - val_loss: 0.2554\n",
      "Epoch 74/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2555 - val_loss: 0.2550\n",
      "Epoch 75/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2551 - val_loss: 0.2546\n",
      "Epoch 76/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2548 - val_loss: 0.2543\n",
      "Epoch 77/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2545 - val_loss: 0.2540\n",
      "Epoch 78/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2542 - val_loss: 0.2537\n",
      "Epoch 79/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2539 - val_loss: 0.2534\n",
      "Epoch 80/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2536 - val_loss: 0.2531\n",
      "Epoch 81/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2533 - val_loss: 0.2528\n",
      "Epoch 82/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2530 - val_loss: 0.2525\n",
      "Epoch 83/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2527 - val_loss: 0.2522\n",
      "Epoch 84/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2524 - val_loss: 0.2518\n",
      "Epoch 85/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2520 - val_loss: 0.2515\n",
      "Epoch 86/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2517 - val_loss: 0.2512\n",
      "Epoch 87/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2514 - val_loss: 0.2508\n",
      "Epoch 88/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2510 - val_loss: 0.2505\n",
      "Epoch 89/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2507 - val_loss: 0.2501\n",
      "Epoch 90/1000\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.2504"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec963292d1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Expected to talk about the components of autoencoder and their purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Autoencoder (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile & fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolutional autoencoder\n",
    "\n",
    "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Create Model \n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.predict(x_train)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will train an autoencoder at some point in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Autoencoders (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
    "\n",
    "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
       "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
       "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
       "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
       "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
       "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
       "        0.23894644,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "    - Enocder\n",
    "    - Decoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "    - Can do in Keras Easily\n",
    "    - Can use a variety of architectures\n",
    "    - Architectures must follow hourglass shape\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "    - Extract just the encoder to use for various tasks\n",
    "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "__References__\n",
    "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
    "\n",
    "__Additional Material__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
