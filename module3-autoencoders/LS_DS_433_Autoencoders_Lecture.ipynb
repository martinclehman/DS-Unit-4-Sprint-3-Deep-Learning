{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Architecture (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "### Extremely Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,)) # do flat representation of image\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim,activation='sigmoid')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784,activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img,decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "\n",
    "# create the decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4653 - val_loss: 0.3308\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.3053 - val_loss: 0.2900\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2838 - val_loss: 0.2790\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2764 - val_loss: 0.2740\n",
      "Epoch 5/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2727 - val_loss: 0.2712\n",
      "Epoch 6/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2705 - val_loss: 0.2695\n",
      "Epoch 7/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2691 - val_loss: 0.2683\n",
      "Epoch 8/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2681 - val_loss: 0.2675\n",
      "Epoch 9/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2674 - val_loss: 0.2668\n",
      "Epoch 10/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2668 - val_loss: 0.2663\n",
      "Epoch 11/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2664 - val_loss: 0.2659\n",
      "Epoch 12/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2660 - val_loss: 0.2656\n",
      "Epoch 13/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2657 - val_loss: 0.2653\n",
      "Epoch 14/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2655 - val_loss: 0.2651\n",
      "Epoch 15/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2652 - val_loss: 0.2649\n",
      "Epoch 16/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2651 - val_loss: 0.2647\n",
      "Epoch 17/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2649 - val_loss: 0.2646\n",
      "Epoch 18/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2648 - val_loss: 0.2644\n",
      "Epoch 19/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2646 - val_loss: 0.2643\n",
      "Epoch 20/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2645 - val_loss: 0.2642\n",
      "Epoch 21/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2644 - val_loss: 0.2641\n",
      "Epoch 22/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2643 - val_loss: 0.2640\n",
      "Epoch 23/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2643 - val_loss: 0.2640\n",
      "Epoch 24/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2642 - val_loss: 0.2639\n",
      "Epoch 25/1000\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2641 - val_loss: 0.2638\n",
      "Epoch 26/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2641 - val_loss: 0.2638\n",
      "Epoch 27/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2640 - val_loss: 0.2637\n",
      "Epoch 28/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2640 - val_loss: 0.2637\n",
      "Epoch 29/1000\n",
      "25344/60000 [===========>..................] - ETA: 1s - loss: 0.2639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec963292d1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Expected to talk about the components of autoencoder and their purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Autoencoder (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))  # <- This is first layer of our network\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded) # -> Fully dehydrated layer\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3335 - val_loss: 0.2484\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2158 - val_loss: 0.1815\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1723 - val_loss: 0.1616\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1565 - val_loss: 0.1480\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1431 - val_loss: 0.1371\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1351 - val_loss: 0.1303\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1293 - val_loss: 0.1255\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1249 - val_loss: 0.1213\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1215 - val_loss: 0.1183\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.1186 - val_loss: 0.1157\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1163 - val_loss: 0.1141\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1146 - val_loss: 0.1120\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1129 - val_loss: 0.1107\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1115 - val_loss: 0.1093\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1100 - val_loss: 0.1080\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1089 - val_loss: 0.1070\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1078 - val_loss: 0.1058\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1068 - val_loss: 0.1049\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1058 - val_loss: 0.1042\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1050 - val_loss: 0.1032\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1043 - val_loss: 0.1026\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1027 - val_loss: 0.1012\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1021 - val_loss: 0.1005\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1014 - val_loss: 0.0999\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1007 - val_loss: 0.0992\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1001 - val_loss: 0.0988\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0995 - val_loss: 0.0981\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0989 - val_loss: 0.0977\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0983 - val_loss: 0.0969\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0977 - val_loss: 0.0964\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0971 - val_loss: 0.0959\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0967 - val_loss: 0.0953\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0961 - val_loss: 0.0948\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0957 - val_loss: 0.0946\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0952 - val_loss: 0.0941\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0948 - val_loss: 0.0940\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0946 - val_loss: 0.0933\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0941 - val_loss: 0.0931\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0937 - val_loss: 0.0927\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0934 - val_loss: 0.0923\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0924 - val_loss: 0.0914\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0920 - val_loss: 0.0911\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0918 - val_loss: 0.0909\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0916 - val_loss: 0.0912\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0913 - val_loss: 0.0906\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0910 - val_loss: 0.0900\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0908 - val_loss: 0.0899\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0905 - val_loss: 0.0898\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0904 - val_loss: 0.0898\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0901 - val_loss: 0.0894\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0899 - val_loss: 0.0892\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0897 - val_loss: 0.0891\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0896 - val_loss: 0.0887\n",
      "Epoch 57/500\n",
      " 5488/60000 [=>............................] - ETA: 2s - loss: 0.0893"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a37e1c97b3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile & fit model\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=500,\n",
    "                batch_size=784,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolutional autoencoder\n",
    "\n",
    "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Create Model \n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.predict(x_train)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will train an autoencoder at some point in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Autoencoders (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
    "\n",
    "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
       "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
       "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
       "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
       "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
       "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
       "        0.23894644,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "    - Enocder\n",
    "    - Decoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "    - Can do in Keras Easily\n",
    "    - Can use a variety of architectures\n",
    "    - Architectures must follow hourglass shape\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "    - Extract just the encoder to use for various tasks\n",
    "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "__References__\n",
    "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
    "\n",
    "__Additional Material__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
